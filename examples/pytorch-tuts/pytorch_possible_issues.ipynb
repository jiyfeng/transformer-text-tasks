{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(10, requires_grad=True)\n",
    "b = torch.rand(10, requires_grad=True)\n",
    "scalar = (a+b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "assert a.grad is None # before backwards\n",
    "scalar.backward()\n",
    "assert a.grad is not None\n",
    "assert b.grad is not None\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No computational graph is required to be built in PyTorch. Everything is centered around tensor operations.\n",
    "\n",
    "#### Building the computational graph is an expensive operation, and hogs up both memory and computational resources. So if possible, you want to disable the computational graph when you don't need gradients. \n",
    "#### For example, at inference time you just need the model outputs but never backpropagate through any losses. In such cases, you can use the context manager torch.no_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run\n",
    "with torch.no_grad():\n",
    "    predictions = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-place operations modify the contents of a tensor without copying the memory contents. These operations do not create new variables in the computational graph so cannot be backpropagated through. PyTorch raises an error while trying to build the computational graph.\n",
    "#### One thing to be wary of is the += operator: this actually triggers an in-place operation, following the behavior of numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: a leaf Variable that requires grad has been used in an in-place operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU support - you can move tensors to the GPU by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run\n",
    "\n",
    "tensor.cuda()\n",
    "# OR\n",
    "tensor.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function ensures that x and t are on the same device, regardless of where x is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function(x):\n",
    "    t = torch.zeros(10).to(x.device)\n",
    "    some_operation(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuntimeError: CUDA Error: out of memory\n",
    "\n",
    "#### There are multiple possible causes for this error, the most common ones being:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Batch sizes are too large - Although model weights do take up a lot of memory, the primary memory hog during training is the intermediate activations. This is because the forward activations are typically stored for backpropagation and take up much more space since the memory usage of the activations scales linearlly with the batch size.\n",
    "#### Solution: Using smaller batch sizes generally solves the memory issue, but can cause training to become unstable. You can solve this problem using gradient accumulation, where you compute the gradient for multiple mini-batches before running the optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2: Need to restart the kernel if OOM error is encountered.\n",
    "#### Solution: Use the code here to create a decorator that precludes you from having to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    " \n",
    "def get_ref_free_exc_info():\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    " \n",
    "def gpu_mem_restore(func):\n",
    "    \"\"\"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gpu_mem_restore\n",
    "def do_something_on_the_gpu():\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU envts\n",
    "\n",
    "Sometimes you want to constrain which process usees which GPU. The easiest way to do this is to set os.environ[\"CUDA_VISIBLE_DEVICES\"] to the GPU(s) you want the process to be able to see. This will disable PyTorch from even knowing the existence of other GPUs. \n",
    "\n",
    "Another way is to use torch.device(\"cuda:i\") to manually select the device in your code, which is effective if you need fine-grained control over which tensor is in which device (e.g. with multi-GPU training). \n",
    "\n",
    "A word of caution: torch.device(\"cuda:0\") maps to the first device PyTorch sees, so if you set os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" the device will map to GPU # 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA version compaitibility\n",
    "\n",
    "You can check for which cuda versions are supported by each version of PyTorch here https://pytorch.org/get-started/previous-versions/.\n",
    "\n",
    "In PyTorch, you can check whether PyTorch thinks it has access to GPUs via the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving things to the GPU and back\n",
    "\n",
    "Often, you will want to move batches of tensors or dictionaries of tensors to the GPU/CPU. \n",
    "\n",
    "A utility function I find useful is to_device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(x, device: torch.device):\n",
    "    \"\"\"Transfers tensor or collection of tensors to a device.\"\"\"\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return type(x)(to_device(v, device) for v in x)\n",
    "    elif isinstance(x, dict):\n",
    "        return {k: to_device(v, device) for k, v in x.items()}\n",
    "    else:\n",
    "        return x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting numpy arrays to tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros(10)\n",
    "b = np.zeros(10, dtype=np.int64)\n",
    "\n",
    "# Converting to Tensor converts ints to floats\n",
    "print(torch.Tensor(a))\n",
    "print(torch.Tensor(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# To convert to int tensor use lowercase tensor with dtype int\n",
    "print(torch.tensor(b, dtype=torch.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE Function Torch.from_numpy for tensors that won't change\n",
    "This function takes advantage of the fact that PyTorch and numpy can share the same underlying memory layout, so as long as you don't change the data type, the conversion does not require any memory copying. \n",
    "\n",
    "### torch.form_numpy is MUCH faster than torch.Tensor since it doesn't create a copy.\n",
    "\n",
    "Be careful though, because torch.from_numpy shares the same underlying data, so any modifications to the tensor will propagate back to the numpy array (and vice-versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting tensors to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run\n",
    "a.cpu().detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
